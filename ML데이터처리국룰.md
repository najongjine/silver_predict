# ML 데이터 처리 국룰 (Machine Learning Data Processing Standard)

Q: PDF에 적힌 것만 전부인가요?
A: **아니요, 절대 아닙니다.** 일반적인 요약 문서들은 "이상적인 상황"을 가정한 입문용인 경우가 많습니다. 실제 현업(Real World) 데이터 사이언스는 데이터가 훨씬 더럽고 복잡하며, 상황에 따라 수십 가지의 다른 대처법이 필요한 "끝없는 최적화 과정"입니다.

아래에 보편적인 국룰(Standard)과 상황별/심화 내용을 **초등학생 비유**부터 **빅데이터 이론**까지 정리해 드립니다.

---

## 1. 초등학생도 이해하는 쉬운 설명: "최고의 쉐프와 요리 재료"

머신러닝(ML) 모델을 훈련시키는 것은 **"최고의 쉐프(AI)"에게 "요리(예측)"를 가르치는 과정**과 완벽히 똑같습니다.

여기서 **데이터 = 요리 재료(식재료)**입니다.

### 왜 전처리를 해야 해요?
밭에서 갓 뽑은 **당근(Raw Data)**을 흙도 안 털고 통째로 냄비에 넣으면 어떻게 될까요? 요리는 흙맛이 나고 망하겠죠.

1.  **세척 (Cleaning):** 묻어있는 흙(이상한 값)을 털어내고, 썩은 부분(결측치)은 도려내야 합니다.
2.  **손질 (Transformation):** 감자는 깎고, 당근은 채 썰어야 합니다. 쉐프(AI)가 칼질을 못해서, 우리가 미리 **한입 크기(0~1 사이 숫자)**로 다 썰어서 줘야 합니다.
3.  **조합 (Feature Engineering):** 맹물보다는 육수를 내서 주면 더 맛있겠죠? 재료를 섞어서 더 좋은 맛을 내는 과정입니다.

**결론:** 재료 손질이 엉망이면, 백종원(최고의 모델)이 와도 요리는 맛이 없습니다.

---

## 2. 컴퓨터공학 및 빅데이터 이론 연결

위의 비유를 전공 용어로 바꾸면 다음과 같습니다.

### GIGO 원칙 (Garbage In, Garbage Out)
*"쓰레기가 들어가면 쓰레기가 나온다."*
데이터 처리의 제1원칙입니다. 아무리 고도화된 딥러닝 알고리즘(Transformer, XGBoost 등)을 사용하더라도, 입력 데이터의 품질(Data Quality)이 낮으면 모델 성능(Accuracy)은 절대 확보할 수 없습니다.

실제 프로젝트 시간의 **80%는 데이터 전처리(Preprocessing)**에 쓰이고, 모델링은 20%에 불과합니다.

---

## 3. 보편적인 데이터 처리 "국룰" 프로세스 (The Standard Pipeline)

데이터 사이언스에서 가장 널리 쓰이는 표준 절차입니다.

### 1단계: 데이터 수집 (Collection)
*   DB 쿼리, API 호출, 웹 크롤링 등을 통해 Raw Data를 확보합니다.

### 2단계: 탐색적 데이터 분석 (EDA - Exploratory Data Analysis)
*   데이터의 생김새를 눈으로 확인하는 과정.
*   **시각화:** 히스토그램, 산점도(Scatter plot)를 그려 분포 확인.
*   **통계량:** 평균(Mean), 중앙값(Median), 표준편차(Std) 확인.

### 3단계: 데이터 정제 (Data Cleaning) - *가장 중요*
가장 흔하게 발생하는 두 가지 문제를 해결합니다.

1.  **결측치 (Missing Values / Null) 처리:**
    *   **삭제(Drop):** 데이터가 수십만 건으로 충분하다면, 빈 값이 있는 행을 그냥 지웁니다.
    *   **대체(Imputation) - 국룰:**
        *   수치형: 평균(Mean)이나 중앙값(Median)으로 채움.
        *   범주형: 최빈값(Mode)으로 채움.
        *   *고수 레벨:* 다른 변수들을 이용해 머신러닝으로 빈 값을 예측해서 채움 (MICE, KNN Imputer).

2.  **이상치 (Outlier) 처리:**
    *   데이터 분포에서 동떨어진 값 (예: 키 300cm, 연봉 -100원).
    *   **IQR 방식:** 1분위수와 3분위수 범위를 벗어나면 제거.
    *   **Z-score:** 평균에서 표준편차의 3배 이상 떨어지면 제거.

### 4단계: 데이터 변환 (Data Transformation)
컴퓨터는 숫자만 이해하고, 큰 숫자에 민감합니다.

1.  **인코딩 (Encoding): 문자를 숫자로**
    *   **Label Encoding:** 사과=1, 배=2 (순서가 생기는 단점 있음).
    *   **One-Hot Encoding (국룰):** 사과=[1,0], 배=[0,1] (순서 없이 독립적).

2.  **스케일링 (Scaling): 숫자의 범위를 맞춤**
    *   키(170cm)와 몸무게(60kg)는 단위가 다릅니다. AI는 170이 60보다 크니까 키가 더 중요하다고 착각합니다.
    *   **Min-Max Scaler:** 모든 값을 0~1 사이로 압축.
    *   **Standard Scaler:** 평균 0, 분산 1인 정규분포로 변환 (이상치에 강함).

### 5단계: 데이터 분할 (Splitting)
*   **Train Set (학습용):** 60~80% (공부 교과서)
*   **Validation Set (검증용):** 10~20% (모의고사)
*   **Test Set (평가용):** 10~20% (수능 시험지 - 절대 학습에 쓰면 안 됨)

---

## 4. 상황 판별법 & 상황별 대처 기술 (Advanced)

"무조건 이렇게 해라"는 없습니다. 상황(Context)에 따라 무기를 골라 써야 합니다.

### 상황 A: 데이터 개수가 너무 적을 때 (Small Data)
*   **판별:** 데이터가 수백 개 수준이거나, 모델이 학습 데이터를 다 외워버리는(Overfitting) 현상 발생.
*   **대처법:**
    1.  **Data Augmentation (데이터 증강):** 
        *   이미지라면 돌리고(Rotate), 자르고(Crop), 뒤집어서(Flip) 개수를 뻥튀기합니다.
        *   텍스트라면 번역했다가 다시 번역하기(Back-translation) 등을 사용.
    2.  **K-Fold Cross Validation (교차 검증):**
        *   데이터를 5조각 내서, 4조각 공부+1조각 시험을 5번 돌아가며 반복. 적은 데이터를 알뜰하게 씁니다.
    3.  **Transfer Learning (전이 학습):**
        *   이미 똑똑한 모델(Pre-trained Model)을 가져와서 내 데이터로 조금만 더 가르칩니다.

### 상황 B: 특징(Feature)이 너무 많을 때 (High Dimensionality)
*   **판별:** 엑셀로 쳤을 때 열(Column)이 수백, 수천 개일 때. "차원의 저주"에 걸려 성능이 급락함.
*   **대처법:**
    1.  **Feature Selection:** 상관관계 분석(Correlation)을 통해 결과와 관련 없는 변수는 과감히 삭제.
    2.  **차원 축소 (Dimensionality Reduction):**
        *   **PCA (주성분 분석):** 여러 변수를 합쳐서 핵심 정보만 남기는 수학적 압축 기법.

### 상황 C: 데이터 비율이 깨져있을 때 (Imbalanced Data)
*   **판별:** 암 환자 분류 문제에서 (암 환자 3명, 정상 97명)인 경우. 모델이 무조건 "정상"이라고 찍어도 97점 맞음.
*   **대처법:**
    1.  **Oversampling (SMOTE):** 적은 쪽 데이터(암 환자)를 인공적으로 생성해서 늘림. (가장 보편적)
    2.  **Undersampling:** 많은 쪽 데이터(정상)를 줄여서 비율을 맞춤. (데이터 손실 위험)
    3.  **Stratified Split:** 데이터를 나눌 때 비율을 유지하면서 나누도록 강제함.

### 상황 D: 데이터 분포가 한쪽으로 쏠렸을 때 (Skewed)
*   **판별:** 소득, 집값 데이터처럼 대부분 낮은 값에 몰려있고 일부 슈퍼 리치가 있는 경우 (Long Tail).
*   **대처법:**
    *   **Log Transformation (로그 변환):** `np.log1p()`를 씌우면 쏠린 분포가 예쁜 종 모양(정규분포)으로 펴집니다. 모델 학습이 훨씬 잘 됩니다.

---

### 🔥 최종 요약 (이것만 기억하면 "국룰" 마스터)

1.  **무조건 맛을 봐라 (EDA):** 데이터를 안 보고 모델링하는 건 눈 감고 운전하는 것.
2.  **빈 건 채우고 튀는 건 자른다 (Missing & Outlier):** 평균 대치, IQR 제거.
3.  **숫자는 줄 세우고 문자는 찢는다 (Scaling & One-Hot):** 스케일러 필수, 원핫인코딩 필수.
4.  **비율이 안 맞으면 맞춘다 (Sampling):** SMOTE 등 활용.
5.  **검증은 확실하게 (Cross Validation):** 우연히 점수가 잘 나온 건지 확인 필수.

이 내용을 이해하고 적용한다면, 학부 수준의 데이터 사이언스 이론은 마스터한 셈입니다.

---

## 5. 자주 묻는 질문 (FAQ)

### Q: EDA(탐색적 데이터 분석)를 할 때 Y(정답, 라벨)값만 분석하면 되나요? 아니면 각 Feature(X)마다 다 해줘야 하나요?

**A: 반드시 "전부 다" 해야 합니다. (X, Y, 그리고 관계까지)**

Y만 보는 것은 시험 성적 분포만 확인하고, 어떤 과목(X)을 잘해서 성적이 잘 나왔는지 분석하지 않는 것과 같습니다. 다음 3가지를 꼭 확인해야 합니다.

1.  **Target (Y) 분석 (필수):**
    *   **목적:** 내가 맞춰야 할 정답의 분포를 확인.
    *   **확인할 점:**
        *   데이터 불균형(Class Imbalance)이 있는가? (암환자 1% vs 정상 99%) -> 비율이 깨져있다면 나중에 SMOTE 등을 써야 함을 미리 파악.
        *   분포 모양이 종 모양(Normal Distribution)인가? -> 쏠려있다면 로그 변환 필요.

2.  **Feature (X) 개별 분석 (Univariate Analysis):**
    *   **목적:** 재료 하나하나의 상태 점검.
    *   **확인할 점:**
        *   **이상치(Outlier):** "나이" 변수에 200살이 들어있진 않은가?
        *   **결측치(Missing):** "주소" 변수에 빈칸이 90%라면 이 변수는 버려야 할 수도 있음.
        *   **데이터 타입:** 숫자인 줄 알았는데 문자열("1,000")로 들어있는지 확인.

3.  **X와 Y의 관계 분석 (Bivariate Analysis) - *가장 중요*:**
    *   **목적:** "어떤 재료가 맛(Y)을 결정하는가?"를 파악.
    *   **확인할 점:**
        *   "키(X)"가 클수록 "몸무게(Y)"도 늘어나는가? (상관관계 분석)
        *   특정 변수(X)가 결과(Y)에 아무런 영향이 없다면, 나중에 모델 학습에서 과감히 뺄 수도 있음 (Feature Selection의 기초).

**결론:** 
*   **Y만 분석:** 반쪽짜리도 안 되는 분석.
*   **X, Y, X-Y 관계 모두 분석:** 정석적인 EDA. 모델 성능을 올리는 인사이트는 여기서 나옵니다.

---

## 6. [실전] 상황별 그래프 그리는 법 공략집 (Cheat Sheet)

질문하신 내용(**"X1+Y, X2+Y 이렇게 보는 게 맞냐?"**)이 정확합니다. 이를 전문 용어로 **이변량 분석(Bivariate Analysis)**이라고 합니다. 데이터 타입에 따라 그려야 할 그래프가 정해져 있습니다.

### 1. X 혼자 볼 때 (Univariate) - "재료 상태 확인"
*   **수치형 (키, 몸무게):**
    *   **Histogram (히스토그램):** 데이터가 어디에 쏠려있는지 확인.
    *   **Boxplot:** 이상치(튀는 값)가 있는지 확인.
*   **범주형 (성별, 지역):**
    *   **Bar Chart (막대그래프):** 데이터 개수(Count) 확인.

### 2. X와 Y를 같이 볼 때 (Bivariate) - "궁합 확인"
가장 중요한 부분입니다. **X(원인)와 Y(결과)**의 관계를 봅니다.

| X (원인) | Y (결과) | 추천 그래프 (국룰) | 설명 |
| :--- | :--- | :--- | :--- |
| **숫자** (공부시간) | **숫자** (시험성적) | **Scatter Plot (산점도)** | 점들이 우상향하면 "비례", 우하향하면 "반비례". 가장 중요함. |
| **범주** (성별) | **숫자** (연봉) | **Boxplot (박스플롯)** | 남자의 연봉 박스와 여자의 연봉 박스 위치/길이 차이를 비교. |
| **숫자** (나이) | **범주** (생존여부) | **Histogram (겹쳐그리기)** | 생존자의 나이 분포와 사망자의 나이 분포를 겹쳐서 비교. |
| **범주** (흡연여부) | **범주** (폐암여부) | **Heatmap / Mosaic Plot** | 표(Crosstab)를 만들어서 색깔 진하기로 비율 비교. |

**Tip:** 파이썬(Seaborn)을 쓴다면 `sns.pairplot(data)` 한 줄로 모든 X와 Y의 관계 그래프를 한 번에 다 그려볼 수도 있습니다.

---

### Q: 모든 변수(X)를 전부 종 모양(정규분포)으로 만들고 시작해야 하나요? 아니면 Y만 하면 되나요?

**A: 어떤 모델을 쓰느냐에 따라 다릅니다. (무조건 다 할 필요 없음!)**

초보자들이 가장 많이 하는 오해 중 하나입니다. "데이터는 무조건 정규분포여야 좋다"는 반만 맞는 말입니다.

#### 1. Y 데이터 (정답)
*   **회귀 문제 (숫자 예측):** **Y가 정규분포에 가까울수록 좋습니다.**
    *   예: 집값이 10억, 20억 하는데 가끔 500억짜리가 있다면(Skewed), 모델이 500억을 맞추려고 노력하다가 10억짜리 예측을 다 망칩니다.
    *   **해결:** 로그 변환(`np.log1p`)을 해서 500억을 깎아내려 종 모양으로 만들어주면 성능이 확 오릅니다.
*   **분류 문제 (O/X 예측):** 종 모양이 의미가 없습니다. (클래스 불균형만 맞추면 됨)

#### 2. X 데이터 (재료) - 모델 취향을 탑니다
*   **트리 기반 모델 (Random Forest, XGBoost, LightGBM) - 요새 대세:**
    *   **"신경 안 씀."** 데이터가 종 모양이든 찌그러져 있든 아무 상관 없습니다. 이 모델들은 스무고개 하듯이 "100보다 커? 작아?"로만 판단하기 때문입니다. 굳이 변환할 필요 없습니다.
*   **선형 기반 모델 (Linear Regression, SVM, Deep Learning) - 고전 & 딥러닝:**
    *   **"엄청 따짐."** 데이터가 종 모양이어야 성능이 잘 나옵니다.
    *   이 모델들은 수학적 거리(Distance)나 기울기(Gradient)를 계산하기 때문에, 데이터가 한쪽으로 쏠려 있으면 학습이 잘 안 됩니다. 이때는 정규분포 변환(Log 변환, Power Transformation)이 필수입니다.

*   **XGBoost 쓴다:** 굳이 안 해도 됨. (하면 약간 좋을 수 있으나 필수는 아님)
*   **딥러닝/선형회귀 쓴다:** X, Y 둘 다 최대한 종 모양으로 만들어줘야 함.

---

### Q: Label Data(Y)나 Group Data(범주형 X)의 카운트(개수)가 서로 비슷해야 하나요?

**A: Y(정답)는 "비슷할수록 좋고", X(재료)는 "크게 상관없습니다."**

#### 1. Label Data (Y, 정답) -> "밸런스가 생명"
*   **이상향:** A클래스 50개 : B클래스 50개 (5:5 황금밸런스).
*   **현실:** 암환자(1) vs 정상인(99).
*   **왜 비슷해야 하나요?**
    *   99:1로 데이터가 있으면, 모델은 공부를 안 하고 무조건 "정상"이라고 찍어도 99점을 맞습니다. (정확도의 함정)
    *   따라서 **SMOTE(데이터 뻥튀기)**나 **Class Weight(소수 데이터에 가점 주기)** 기술을 써서 모델이 소수 클래스(1)를 무시하지 못하게 강제해야 합니다.

#### 2. Group Data (X, 범주형 변수) -> "달라도 괜찮음"
*   **상황:** "거주지역" 변수에 서울(10,000명), 부산(500명), 제주(50명).
*   **왜 상관없나요?**
    *   서울 사람이 많다고 해서 모델 성능이 망가지지는 않습니다. 모델은 서울의 특징, 부산의 특징을 각각 배웁니다.
*   **단, "너무 적으면" 문제입니다 (Rare Label Issue):**
    *   만약 "울릉도"가 딱 1명만 있다면? 모델 입장에서 이 1명만 보고 "울릉도 사람은 이렇구나"라고 일반화하기 힘듭니다.
    *   **해결:** 개수가 너무 적은(예: 1% 미만) 카테고리들은 **"기타(Others)"**라는 하나의 그룹으로 묶어버리는 것이 국룰 기술입니다.


---

## 7. [필살기] Gemini와 협업하여 ML 모델 "뚝딱" 만드는 공식

질문하신 대로입니다. **"님이 주제와 EDA 결과만 던져주면, 나머지는 AI가 코드를 다 짜줍니다."**
가장 효율적으로 협업하는 3단계 프로세스를 알려드립니다.

### 1단계: 상황 브리핑 (Context)
Gemini에게 다음 정보를 명확히 줍니다.
> "나 지금 **[집값 예측(회귀)]** 하려고 해. 데이터는 10만 개고, 컬럼은 **[평수, 위치, 연식]** 등이 있어."

### 2단계: EDA 코드 요청 & 결과 공유 (Interaction)
직접 그래프를 그릴 필요도 없습니다. 코드도 시키세요.
> **나:** "EDA 하게 그래프 그리는 파이썬 코드 짜줘. 국룰대로 짜줘."
> **(Gemini가 코드 줌) -> (님이 실행) -> (결과 통계/그래프를 캡처하거나 텍스트로 복사)**
> **나:** "결과 보니까 '연식' 변수에 결측치가 많고, '집값'이 왼쪽으로 쏠려있네. 이거 확인했지?"

### 3단계: 모델링 요청 (Implementation)
분석된 정보를 바탕으로 최종 코드를 요청합니다.
> **나:** "방금 본 것처럼 결측치는 평균으로 채우고, 집값은 로그 변환해서 **XGBoost**로 돌려보자.
> 성능 평가(Validation)까지 포함해서 파이썬 코드 전체 작성해줘."

**핵심:** 님은 **"데이터의 상태를 보고(EDA), 무엇을 할지 결정(Decision)"**만 하면 됩니다. 복잡한 코딩(구현)은 전부 AI가 합니다. 이것이 현존하는 가장 빠른 ML 개발 방식입니다.



