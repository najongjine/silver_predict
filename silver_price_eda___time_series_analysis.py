# -*- coding: utf-8 -*-
"""Silver_Price_EDA | Time_series_Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/wildojisan/silver-price-eda-time-series-analysis.33b39ef1-4219-4ff6-b0f5-fc093c28837b.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20260120/auto/storage/goog4_request%26X-Goog-Date%3D20260120T024658Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7a2a5886044af944f92d77767573b4e27430be74e7b7272340264ea80b9d3ed58081715626a08138e8ebaadc2c7a4ec0b170e244510775810c5a9fea20c29a20cf2907bbc0eb948da58417cbb44921d249e1579f5e3a5ca71c5b16b40ad7d8999b0f72c5d817f2b0eb46cc5948a071b4ff2585bfcf35803d93583ce2bac6caeda4f1d531976da4329600f3f3e70d30508351ff6391c4879043706fc5c7051b5a5c97efc8efd0bfd99990cbd5b755788c4a217f4f2ba8ae00279260f0457ee90001958bf2d82cfdba1f2232233e4c60df9803834c5bcdfa1979325b3771052ecc8efdbdca2354a5807febcb65e3617a7d732111f79f790c68e9f4c97ec91373b8
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
muhammadaammartufail_silver_prices_10_year_data_and_2026_forecast_path = kagglehub.dataset_download('muhammadaammartufail/silver-prices-10-year-data-and-2026-forecast')

print('Data source import complete.')

"""---
# **Silver Price Time Series Analysis: Comprehensive EDA & Forecasting**
---

**Instructor:** Sir Aammar Tufail

**Author name:** Faryal Rifaz

**Date:**  jan 2026

**Connect on LinkedIn:** [https://www.linkedin.com/in/faryal-rifaz/]

**Connect on GitHub:** [https://github.com/faryalrifaz3374]

**Don't forget to upvote the notebook and follow me**

## ðŸŽ¯ **Introduction & Objectives**

        
### ðŸ“‹ Project Overview
        
**Silver**, often called "the poor man's gold," is one of the most fascinating precious metals in the financial markets. Unlike gold, silver has significant industrial applications alongside its monetary role, making its price dynamics particularly complex and intriguing. This comprehensive analysis explores 10 years of silver price data (2016-2026) to uncover hidden patterns, build predictive models, and provide actionable insights for investors, traders, and data science enthusiasts.
        
In recent years, silver has experienced unprecedented volatility and growth, reaching historical highs that have captured the attention of both institutional and retail investors. This analysis aims to decode the mathematical patterns behind these price movements using cutting-edge time series analysis techniques and machine learning algorithms.

### ðŸš€ Why This Analysis Matters
        
**For Investors:**
- Understand silver price dynamics and market drivers
- Identify optimal entry and exit points based on statistical patterns
- Quantify risk through volatility analysis and confidence intervals
- Gain data-driven insights for portfolio allocation decisions
        
**For Traders:**
- Discover seasonal patterns and cyclical behaviors
- Build predictive models for short-term price movements
- Understand market sentiment through technical indicators
- Develop algorithmic trading strategies based on statistical evidence
        
**For Data Scientists:**
- Master time series analysis techniques with real-world data
- Learn feature engineering for financial forecasting
- Compare multiple ML approaches for time series prediction
- Understand stationarity, autocorrelation, and model diagnostics

## ðŸ“Š **Dataset Description**
        
**Source:** Kaggle Dataset - Silver Prices 10-Year Data & 2026 Forecast
        
**Time Period:** January 19, 2016 - January 15, 2026 (2,513 trading days)
        
**Key Variables:**
- **Price:** Daily closing price of silver (USD per troy ounce)
- **High/Low:** Daily price range indicators
- **Volume:** Trading volume (when available)
- **Date:** Trading date information
        
**Data Quality:** High-quality daily data with minimal missing values, providing an excellent foundation for robust statistical analysis.

## ðŸŽ¯ **Primary Objectives**
        
#### 1. **Exploratory Data Analysis (EDA)**
- [ ] Visualize silver price trends and identify long-term patterns
- [ ] Analyze price distribution and detect outliers
- [ ] Examine seasonal effects and cyclical behaviors
- [ ] Calculate and interpret volatility measures
- [ ] Identify structural breaks and regime changes
        
#### 2. **Statistical Time Series Analysis**
- [ ] Test for stationarity using Augmented Dickey-Fuller (ADF) test
- [ ] Perform seasonal decomposition to isolate trend, seasonal, and residual components
- [ ] Analyze autocorrelation and partial autocorrelation functions
- [ ] Identify appropriate ARIMA model parameters
- [ ] Test for normality and randomness in residuals
        
#### 3. **Advanced Feature Engineering**
- [ ] Create lag features (1-30 days) for temporal dependencies
- [ ] Calculate rolling statistics (moving averages, volatility)
- [ ] Develop technical indicators (RSI, MACD, Bollinger Bands)
- [ ] Extract cyclical features (day of year, month patterns)
- [ ] Generate interaction features for enhanced predictive power
        
#### 4. **Machine Learning Model Development**
- [ ] Implement multiple algorithms: Linear Regression, Random Forest, XGBoost
- [ ] Compare model performance using cross-validation
- [ ] Optimize hyperparameters for best performance
- [ ] Analyze feature importance to understand price drivers
- [ ] Validate model robustness through backtesting
        
#### 5. **Forecasting & Prediction**
- [ ] Generate 30-day price forecasts with confidence intervals
- [ ] Quantify prediction uncertainty and model risk
- [ ] Compare different forecasting approaches
- [ ] Provide actionable trading and investment insights
- [ ] Create visual forecasts with uncertainty bands
        
#### 6. **Risk Assessment & Model Validation**
- [ ] Conduct residual analysis and diagnostic tests
- [ ] Calculate Value-at-Risk (VaR) estimates
- [ ] Perform stress testing on model predictions
- [ ] Validate model assumptions and limitations
- [ ] Provide recommendations for model usage

###  **Let's Begin the Analysis!**
        
Now that we've established our objectives and methodology, let's dive into the data and uncover the hidden patterns in silver price movements. Each section builds upon the previous one, creating a comprehensive understanding of this precious metal's market dynamics.

## **Step 1: Import Required Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Time Series Analysis Libraries
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Machine Learning Libraries
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Plotting Configuration
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (15, 8)
plt.rcParams['font.size'] = 12

"""## **Step 2: Load and Validate Data**"""

# Step 2: Load and Validate Data
print("="*60)
print("SILVER PRICE TIME SERIES ANALYSIS")
print("="*60)

# Load the dataset
df = pd.read_csv('/kaggle/input/silver-prices-10-year-data-and-2026-forecast/silver_prices_data.csv')

# Basic information about the dataset
print(f"Dataset Shape: {df.shape}")
print(f"Date Range: {df['Date'].min()} to {df['Date'].max()}")
print(f"Total Trading Days: {len(df)}")
print("\nColumn Information:")
print(df.info())
print("\nMissing Values:")
print(df.isnull().sum())
print("\nDuplicated Rows:", df.duplicated().sum())
df

"""## **Step 3: Data Preprocessing and Feature Engineering**"""

# Step 3: Enhanced Data Preprocessing and Feature Engineering
print("STEP 3: DATA PREPROCESSING AND FEATURE ENGINEERING")
print("="*60)

# Convert Date to datetime
df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values('Date').reset_index(drop=True)

print(f"âœ… Date conversion completed")
print(f"âœ… Data sorted chronologically")

# Create time-based features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['Quarter'] = df['Date'].dt.quarter
df['DayOfYear'] = df['Date'].dt.dayofyear

print(f"âœ… Time-based features created")

# Handle Open column (convert to float)
df['Open'] = df['Open'].astype(float)

# Calculate price differences and returns
df['Price_Change'] = df['Price'].diff()
df['Price_Returns'] = df['Price'].pct_change()
df['High_Low_Spread'] = df['High'] - df['Low']
df['Close_Open_Spread'] = df['Close'] - df['Open']
df['Price_Range'] = df['High'] - df['Low']
df['Price_Body'] = abs(df['Close'] - df['Open'])

print(f"âœ… Price-based features created")

# Moving averages for trend analysis (with min_periods to handle edge cases)
df['MA_7'] = df['Price'].rolling(window=7, min_periods=1).mean()
df['MA_30'] = df['Price'].rolling(window=30, min_periods=1).mean()
df['MA_90'] = df['Price'].rolling(window=90, min_periods=1).mean()

# Volatility measures
df['Volatility_30'] = df['Price'].rolling(window=30, min_periods=1).std()
df['Volatility_7'] = df['Price'].rolling(window=7, min_periods=1).std()

print(f"âœ… Moving averages and volatility calculated")

# Handle missing Volume column
if df['Volume'].isnull().all():
    print(f"âš ï¸  Volume column has all missing values - removing it")
    df = df.drop('Volume', axis=1)
else:
    print(f"âœ… Volume column processed")

# Remove rows with NaN values (mainly from first row due to diff/pct_change)
initial_rows = len(df)
df_clean = df.dropna().reset_index(drop=True)
removed_rows = initial_rows - len(df_clean)

print(f"âœ… Missing values handled")
print(f"   - Initial rows: {initial_rows}")
print(f"   - Final rows: {len(df_clean)}")
print(f"   - Removed rows: {removed_rows}")

print(f"\\nðŸ“Š PREPROCESSING SUMMARY:")
print(f"   - Date range: {df_clean['Date'].min()} to {df_clean['Date'].max()}")
print(f"   - Total features: {df_clean.shape[1]}")
print(f"   - New features created: {df_clean.shape[1] - 6}")  # 6 original columns
print(f"   - Price range: ${df_clean['Price'].min():.2f} to ${df_clean['Price'].max():.2f}")

# Display new features
new_features = [col for col in df_clean.columns if col not in ['Date', 'Price', 'Close', 'High', 'Low', 'Open']]
print(f"\\nðŸ†• NEW FEATURES ADDED:")
for i, feature in enumerate(new_features, 1):
    print(f"   {i:2d}. {feature}")

"""## **Step 4: Comprehensive Exploratory Data Analysis (EDA)**

### **4.1: Statistical Summary**
"""

# Step 4.1: Comprehensive Statistical Analysis
print("ðŸ“Š COMPREHENSIVE STATISTICAL SUMMARY")
print("="*70)

# Create a comprehensive summary
summary_data = {
    'Metric': [
        'Total Observations',
        'Date Range',
        'Price Range ($)',
        'Average Price ($)',
        'Median Price ($)',
        'Standard Deviation ($)',
        'Coefficient of Variation (%)',
        'Skewness',
        'Kurtosis',
        'Minimum Price ($)',
        'Maximum Price ($)',
        'Price Range ($)'
    ],
    'Value': [
        f"{len(df_clean):,}",
        f"{df_clean['Date'].min().strftime('%Y-%m-%d')} to {df_clean['Date'].max().strftime('%Y-%m-%d')}",
        f"{df_clean['Price'].min():.2f} - {df_clean['Price'].max():.2f}",
        f"{df_clean['Price'].mean():.2f}",
        f"{df_clean['Price'].median():.2f}",
        f"{df_clean['Price'].std():.2f}",
        f"{(df_clean['Price'].std() / df_clean['Price'].mean()) * 100:.1f}%",
        f"{df_clean['Price'].skew():.2f}",
        f"{df_clean['Price'].kurtosis():.2f}",
        f"{df_clean['Price'].min():.2f}",
        f"{df_clean['Price'].max():.2f}",
        f"{df_clean['Price'].max() - df_clean['Price'].min():.2f}"
    ]
}

summary_df = pd.DataFrame(summary_data)
print("\nðŸ“ˆ OVERALL DATASET SUMMARY:")
print(summary_df.to_string(index=False))

print(f"\nðŸ“… YEAR-WISE DETAILED ANALYSIS:")
print("-" * 50)
yearly_detailed = df_clean.groupby('Year').agg({
    'Price': ['count', 'mean', 'std', 'min', 'max'],
    'Volatility_30': 'mean',
    'Price_Returns': 'std'
}).round(3)

# Flatten column names
yearly_detailed.columns = ['Trading_Days', 'Avg_Price', 'Price_StdDev', 'Min_Price', 'Max_Price', 'Avg_Volatility', 'Return_Volatility']
yearly_detailed['Price_Range'] = yearly_detailed['Max_Price'] - yearly_detailed['Min_Price']
yearly_detailed['Volatility_Pct'] = (yearly_detailed['Price_StdDev'] / yearly_detailed['Avg_Price'] * 100).round(1)

print(yearly_detailed)

print(f"\nðŸŽ¯ KEY INSIGHTS FROM STATISTICAL ANALYSIS:")
print("-" * 50)
# Calculate some key insights
recent_avg = df_clean[df_clean['Year'] >= 2024]['Price'].mean()
early_avg = df_clean[df_clean['Year'] <= 2017]['Price'].mean()
price_growth = ((recent_avg / early_avg) - 1) * 100

print(f"   â€¢ Price appreciation (2024 vs 2017): {price_growth:.1f}%")
print(f"   â€¢ Most volatile year: {yearly_detailed['Volatility_Pct'].idxmax()} ({yearly_detailed['Volatility_Pct'].max():.1f}%)")
print(f"   â€¢ Least volatile year: {yearly_detailed['Volatility_Pct'].idxmin()} ({yearly_detailed['Volatility_Pct'].min():.1f}%)")
print(f"   â€¢ Highest average price year: ${yearly_detailed['Avg_Price'].idxmax()} (${yearly_detailed['Avg_Price'].max():.2f})")
print(f"   â€¢ Lowest average price year: ${yearly_detailed['Avg_Price'].idxmin()} (${yearly_detailed['Avg_Price'].min():.2f})")

# Identify extreme events
max_price_date = df_clean.loc[df_clean['Price'].idxmax(), 'Date']
min_price_date = df_clean.loc[df_clean['Price'].idxmin(), 'Date']
print(f"   â€¢ Historical peak: ${df_clean['Price'].max():.2f} on {max_price_date.strftime('%B %d, %Y')}")
print(f"   â€¢ Historical trough: ${df_clean['Price'].min():.2f} on {min_price_date.strftime('%B %d, %Y')}")

"""### **4.2: Price Trend Visualization**"""

# Step 4.2: Silver Price Trend Analysis
fig, axes = plt.subplots(2, 2, figsize=(20, 12))

# 1. Overall Price Trend
axes[0,0].plot(df_clean['Date'], df_clean['Price'], linewidth=2, color='silver', alpha=0.8)
axes[0,0].plot(df_clean['Date'], df_clean['MA_30'], linewidth=2, color='red', label='30-day MA')
axes[0,0].plot(df_clean['Date'], df_clean['MA_90'], linewidth=2, color='blue', label='90-day MA')
axes[0,0].set_title('Silver Price Trend with Moving Averages', fontsize=16, fontweight='bold')
axes[0,0].set_xlabel('Date', fontsize=12)
axes[0,0].set_ylabel('Price ($)', fontsize=12)
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# 2. Price Distribution
axes[0,1].hist(df_clean['Price'], bins=50, color='silver', alpha=0.7, edgecolor='black')
axes[0,1].axvline(df_clean['Price'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${df_clean["Price"].mean():.2f}')
axes[0,1].axvline(df_clean['Price'].median(), color='blue', linestyle='--', linewidth=2, label=f'Median: ${df_clean["Price"].median():.2f}')
axes[0,1].set_title('Silver Price Distribution', fontsize=16, fontweight='bold')
axes[0,1].set_xlabel('Price ($)', fontsize=12)
axes[0,1].set_ylabel('Frequency', fontsize=12)
axes[0,1].legend()

# 3. Yearly Price Trends
yearly_data = df_clean.groupby('Year')['Price'].mean()
axes[1,0].bar(yearly_data.index, yearly_data.values, color='silver', alpha=0.8, edgecolor='black')
axes[1,0].set_title('Average Silver Price by Year', fontsize=16, fontweight='bold')
axes[1,0].set_xlabel('Year', fontsize=12)
axes[1,0].set_ylabel('Average Price ($)', fontsize=12)
axes[1,0].tick_params(axis='x', rotation=45)

# 4. Monthly Seasonality
monthly_data = df_clean.groupby('Month')['Price'].mean()
axes[1,1].plot(monthly_data.index, monthly_data.values, marker='o', linewidth=3, markersize=8, color='silver')
axes[1,1].set_title('Average Silver Price by Month (Seasonality)', fontsize=16, fontweight='bold')
axes[1,1].set_xlabel('Month', fontsize=12)
axes[1,1].set_ylabel('Average Price ($)', fontsize=12)
axes[1,1].set_xticks(range(1, 13))
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""### **4.3: Volatility Analysis**"""

# Step 4.3: Volatility Analysis
fig, axes = plt.subplots(2, 2, figsize=(20, 12))

# 1. Price Returns Distribution
axes[0,0].hist(df_clean['Price_Returns'].dropna(), bins=100, color='silver', alpha=0.7, edgecolor='black')
axes[0,0].set_title('Silver Price Returns Distribution', fontsize=16, fontweight='bold')
axes[0,0].set_xlabel('Daily Returns', fontsize=12)
axes[0,0].set_ylabel('Frequency', fontsize=12)
axes[0,0].axvline(0, color='red', linestyle='--', alpha=0.7)

# 2. Rolling Volatility
axes[0,1].plot(df_clean['Date'], df_clean['Volatility_30'], color='red', linewidth=2)
axes[0,1].set_title('30-Day Rolling Volatility', fontsize=16, fontweight='bold')
axes[0,1].set_xlabel('Date', fontsize=12)
axes[0,1].set_ylabel('Volatility ($)', fontsize=12)
axes[0,1].grid(True, alpha=0.3)

# 3. High-Low Spread Analysis
axes[1,0].plot(df_clean['Date'], df_clean['High_Low_Spread'], color='blue', alpha=0.6, linewidth=1)
axes[1,0].set_title('Daily High-Low Spread', fontsize=16, fontweight='bold')
axes[1,0].set_xlabel('Date', fontsize=12)
axes[1,0].set_ylabel('Spread ($)', fontsize=12)
axes[1,0].grid(True, alpha=0.3)

# 4. Price vs Volatility Scatter
valid_data = df_clean.dropna(subset=['Volatility_30'])
axes[1,1].scatter(valid_data['Price'], valid_data['Volatility_30'], alpha=0.5, color='silver', s=20)
axes[1,1].set_title('Price vs Volatility Relationship', fontsize=16, fontweight='bold')
axes[1,1].set_xlabel('Price ($)', fontsize=12)
axes[1,1].set_ylabel('30-Day Volatility', fontsize=12)
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""## **Step 5: Time Series Stationarity Testing (ADF Test)**"""

# Step 5: Augmented Dickey-Fuller (ADF) Test for Stationarity
def adf_test(series, title=''):
    """
    Perform ADF test and return results
    """
    print(f'ADF Test Results for {title}:')
    print('-'*50)

    result = adfuller(series.dropna(), autolag='AIC')

    print(f'ADF Statistic: {result[0]:.6f}')
    print(f'p-value: {result[1]:.6f}')
    print(f'Critical Values:')
    for key, value in result[4].items():
        print(f'\t{key}: {value:.3f}')

    if result[1] <= 0.05:
        print("âœ… Series is stationary (reject null hypothesis)")
    else:
        print("âŒ Series is non-stationary (fail to reject null hypothesis)")
    print('\n')

# Test original price series
print("STATIONARITY TESTING")
print("="*60)
adf_test(df_clean['Price'], 'Original Price Series')

# Test first difference
adf_test(df_clean['Price'].diff(), 'First Difference of Price')

# Test log transformation
adf_test(np.log(df_clean['Price']), 'Log-transformed Price')

# Test log difference
adf_test(np.log(df_clean['Price']).diff(), 'Log Difference of Price')

"""## Step 6: Seasonal Decomposition"""

# Step 6: Time Series Decomposition
# Set Date as index for decomposition
df_ts = df_clean.set_index('Date')['Price']

# Perform seasonal decomposition
decomposition = seasonal_decompose(df_ts, model='additive', period=365)

# Plot decomposition
fig, axes = plt.subplots(4, 1, figsize=(20, 16))

# Original series
axes[0].plot(decomposition.observed, color='silver', linewidth=1.5)
axes[0].set_title('Original Silver Price Series', fontsize=16, fontweight='bold')
axes[0].set_ylabel('Price ($)', fontsize=12)
axes[0].grid(True, alpha=0.3)

# Trend component
axes[1].plot(decomposition.trend, color='red', linewidth=2)
axes[1].set_title('Trend Component', fontsize=16, fontweight='bold')
axes[1].set_ylabel('Trend', fontsize=12)
axes[1].grid(True, alpha=0.3)

# Seasonal component
axes[2].plot(decomposition.seasonal, color='green', linewidth=1)
axes[2].set_title('Seasonal Component', fontsize=16, fontweight='bold')
axes[2].set_ylabel('Seasonal', fontsize=12)
axes[2].grid(True, alpha=0.3)

# Residual component
axes[3].plot(decomposition.resid, color='blue', alpha=0.7, linewidth=0.8)
axes[3].set_title('Residual Component', fontsize=16, fontweight='bold')
axes[3].set_ylabel('Residuals', fontsize=12)
axes[3].set_xlabel('Date', fontsize=12)
axes[3].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Print seasonal statistics
print("SEASONAL COMPONENT ANALYSIS")
print("="*50)
seasonal_strength = np.var(decomposition.seasonal.dropna()) / np.var(decomposition.observed.dropna())
print(f"Seasonal Strength: {seasonal_strength:.4f}")
print("Higher values indicate stronger seasonality")

"""## **Step 7: Autocorrelation Analysis**"""

# Step 7: Autocorrelation and Partial Autocorrelation Analysis
fig, axes = plt.subplots(2, 2, figsize=(20, 12))

# ACF for price level
plot_acf(df_clean['Price'].dropna(), ax=axes[0,0], lags=50, title='ACF - Price Level')
axes[0,0].set_title('Autocorrelation Function - Price Level', fontsize=14, fontweight='bold')

# PACF for price level
plot_pacf(df_clean['Price'].dropna(), ax=axes[0,1], lags=50, title='PACF - Price Level')
axes[0,1].set_title('Partial Autocorrelation Function - Price Level', fontsize=14, fontweight='bold')

# ACF for first difference
plot_acf(df_clean['Price'].diff().dropna(), ax=axes[1,0], lags=50, title='ACF - First Difference')
axes[1,0].set_title('Autocorrelation Function - First Difference', fontsize=14, fontweight='bold')

# PACF for first difference
plot_pacf(df_clean['Price'].diff().dropna(), ax=axes[1,1], lags=50, title='PACF - First Difference')
axes[1,1].set_title('Partial Autocorrelation Function - First Difference', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

"""## **Step 8: Advanced Feature Engineering for Machine Learning**"""

def create_features(df):
    df = df.copy()

    # Lag features
    lags = [1, 3, 5, 7, 10, 14, 21, 30]
    for l in lags:
        df[f'lag_{l}'] = df['Price'].shift(l)

    # Rolling statistics
    windows = [7, 14, 20, 30]
    for w in windows:
        df[f'roll_mean_{w}'] = df['Price'].rolling(w, min_periods=1).mean()
        df[f'roll_std_{w}'] = df['Price'].rolling(w, min_periods=1).std()
        df[f'roll_min_{w}'] = df['Price'].rolling(w, min_periods=1).min()
        df[f'roll_max_{w}'] = df['Price'].rolling(w, min_periods=1).max()

    # RSI
    delta = df['Price'].diff()
    gain = delta.clip(lower=0).rolling(14, min_periods=1).mean()
    loss = (-delta.clip(upper=0)).rolling(14, min_periods=1).mean()
    rs = gain / (loss + 1e-10)
    df['RSI'] = 100 - (100 / (1 + rs))

    # MACD
    ema12 = df['Price'].ewm(span=12, adjust=False).mean()
    ema26 = df['Price'].ewm(span=26, adjust=False).mean()
    df['MACD'] = ema12 - ema26
    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()

    # Bollinger Bands
    df['BB_upper'] = df['roll_mean_20'] + 2 * df['roll_std_20']
    df['BB_lower'] = df['roll_mean_20'] - 2 * df['roll_std_20']
    bb_range = df['BB_upper'] - df['BB_lower']
    df['BB_position'] = np.where(bb_range > 0,
                                 (df['Price'] - df['BB_lower']) / bb_range,
                                 0.5)

    # Date features
    df['DayOfYear'] = df['Date'].dt.dayofyear
    df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)
    df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)
    df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)

    # Trend and momentum
    df['Above_MA20'] = (df['Price'] > df['roll_mean_20']).astype(int)
    df['Above_MA30'] = (df['Price'] > df['roll_mean_30']).astype(int)
    df['Momentum_5'] = df['Price'] / df['lag_5'] - 1
    df['Momentum_10'] = df['Price'] / df['lag_10'] - 1

    return df

"""## **Step 9: Machine Learning Model Development**"""

import numpy as np
import pandas as pd

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb

# ----- Features and target -----
drop_cols = ['Date', 'Price', 'Close', 'High', 'Low', 'Volume']
feature_cols = [c for c in df_ml.columns if c not in drop_cols]

X = df_ml[feature_cols]
y = df_ml['Price']

# ----- Time based split (last 6 months test) -----
split_date = df_ml['Date'].max() - pd.DateOffset(months=6)

train_idx = df_ml['Date'] <= split_date
test_idx  = df_ml['Date'] > split_date

X_train, X_test = X.loc[train_idx], X.loc[test_idx]
y_train, y_test = y.loc[train_idx], y.loc[test_idx]

# ----- Models -----
models = {
    'LinearRegression': LinearRegression(),
    'RandomForest': RandomForestRegressor(
        n_estimators=200,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    ),
    'XGBoost': xgb.XGBRegressor(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        objective='reg:squarederror',
        random_state=42
    )
}

# ----- Training and evaluation -----
results = {}
predictions = {}

for name, model in models.items():

    model.fit(X_train, y_train)

    y_train_pred = model.predict(X_train)
    y_test_pred  = model.predict(X_test)

    results[name] = {
        'Train_RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),
        'Test_RMSE':  np.sqrt(mean_squared_error(y_test, y_test_pred)),
        'Train_MAE':  mean_absolute_error(y_train, y_train_pred),
        'Test_MAE':   mean_absolute_error(y_test, y_test_pred),
        'Train_R2':   r2_score(y_train, y_train_pred),
        'Test_R2':    r2_score(y_test, y_test_pred)
    }

    predictions[name] = y_test_pred

# ----- Results table -----
results_df = pd.DataFrame(results).T.round(4)
print(results_df)

"""## **Step 10: Model Performance Visualization**"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# ---------- Dates for plotting ----------
dates_test = df_ml.loc[test_idx, 'Date']

# ---------- Predictions ----------
predictions = {}
for name, model in models.items():
    predictions[name] = model.predict(X_test)

# ---------- Figure ----------
fig, axes = plt.subplots(1, 3, figsize=(22, 6))

# ---------- 1. Actual vs Predicted ----------
for name, y_pred in predictions.items():
    axes[0].scatter(y_test, y_pred, alpha=0.5, label=name)

axes[0].plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    linestyle='--'
)

axes[0].set_title("Actual vs Predicted Prices")
axes[0].set_xlabel("Actual Price")
axes[0].set_ylabel("Predicted Price")
axes[0].legend()
axes[0].grid(alpha=0.3)

# ---------- 2. Time series comparison ----------
axes[1].plot(dates_test, y_test.values, label="Actual", linewidth=3)

for name, y_pred in predictions.items():
    axes[1].plot(dates_test, y_pred, label=name, linewidth=2)

axes[1].set_title("Predicted vs Actual Prices Over Time")
axes[1].set_xlabel("Date")
axes[1].set_ylabel("Price")
axes[1].legend()
axes[1].grid(alpha=0.3)

# ---------- 3. Model comparison ----------
metrics_df = results_df[['Test_RMSE', 'Test_MAE', 'Test_R2']]
metrics_df.plot(kind='bar', ax=axes[2], alpha=0.8)

axes[2].set_title("Model Performance Comparison")
axes[2].set_xlabel("Models")
axes[2].set_ylabel("Metric Value")
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

"""## **Step 11: Feature Importance Analysis**"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

fig, axes = plt.subplots(1, 3, figsize=(25, 8))

# ---------- Random Forest ----------
rf_model = models['RandomForest']
rf_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=True).tail(15)  # tail for top 15

axes[0].barh(rf_importance['feature'], rf_importance['importance'], color='silver', alpha=0.8)
axes[0].set_title('Random Forest - Top 15 Feature Importance', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Importance', fontsize=12)

# ---------- XGBoost ----------
xgb_model = models['XGBoost']
xgb_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=True).tail(15)

axes[1].barh(xgb_importance['feature'], xgb_importance['importance'], color='gold', alpha=0.8)
axes[1].set_title('XGBoost - Top 15 Feature Importance', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Importance', fontsize=12)

# ---------- Linear Regression ----------
lr_model = models['LinearRegression']
lr_coef = pd.DataFrame({
    'feature': feature_cols,
    'coefficient': np.abs(lr_model.coef_)
}).sort_values('coefficient', ascending=True).tail(15)

axes[2].barh(lr_coef['feature'], lr_coef['coefficient'], color='blue', alpha=0.8)
axes[2].set_title('Linear Regression - Top 15 Coefficients (Absolute)', fontsize=14, fontweight='bold')
axes[2].set_xlabel('Absolute Coefficient Value', fontsize=12)

plt.tight_layout()
plt.show()

# ---------- Print top features ----------
print("TOP 10 MOST IMPORTANT FEATURES")
print("="*50)
print("\nRandom Forest Top 10:")
print(rf_importance.sort_values('importance', ascending=False).head(10).to_string(index=False))
print("\nXGBoost Top 10:")
print(xgb_importance.sort_values('importance', ascending=False).head(10).to_string(index=False))

"""## **Step 12: Forecasting and Future Predictions**"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# ---------- Future Forecasting ----------
best_model = models['XGBoost']

# Future dates
last_date = df_ml['Date'].max()
future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30, freq='D')

# Start from last known row
last_known = df_ml.iloc[-1].copy()
future_rows = []

for i, date in enumerate(future_dates):
    row = last_known.copy()
    row['Date'] = date
    row['DayOfYear'] = date.dayofyear
    row['WeekOfYear'] = date.isocalendar().week
    row['IsMonthStart'] = int(date.is_month_start)
    row['IsMonthEnd'] = int(date.is_month_end)

    # Update lag features
    for lag in [1, 3, 7, 14, 21, 30]:
        col = f'lag_{lag}'
        if i >= lag:
            row[col] = future_rows[i - lag]['Price']
        else:
            row[col] = df_ml['Price'].iloc[-lag + i]

    # Keep rolling stats same as last known row (simplified)
    future_rows.append(row)

df_future = pd.DataFrame(future_rows)

# Predict
X_future = df_future[feature_cols]
future_predictions = best_model.predict(X_future)

# Calculate simple confidence interval from test residuals
residuals = y_test - models['XGBoost'].predict(X_test)
residual_std = np.std(residuals)
conf_int = 1.96 * residual_std  # 95% CI

# ---------- Plot ----------
fig, axes = plt.subplots(2, 1, figsize=(20, 12))

# Historical + test + future
axes[0].plot(df_ml['Date'], df_ml['Price'], label='Historical', color='black', linewidth=2)
axes[0].plot(dates_test, models['XGBoost'].predict(X_test), label='Test Predictions', color='blue', alpha=0.8)
axes[0].plot(future_dates, future_predictions, label='Future Predictions', color='red', linestyle='--', linewidth=3)
axes[0].fill_between(future_dates,
                     future_predictions - conf_int,
                     future_predictions + conf_int,
                     color='red', alpha=0.3, label='95% CI')
axes[0].set_title('Silver Price: Historical & Future Predictions', fontsize=16, fontweight='bold')
axes[0].set_xlabel('Date')
axes[0].set_ylabel('Price ($)')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Future 30-day detailed
axes[1].plot(future_dates, future_predictions, marker='o', linewidth=3, markersize=6, color='red')
axes[1].fill_between(future_dates,
                     future_predictions - conf_int,
                     future_predictions + conf_int,
                     color='red', alpha=0.3)
axes[1].set_title('30-Day Silver Price Forecast', fontsize=16, fontweight='bold')
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Predicted Price ($)')
axes[1].grid(alpha=0.3)

# Annotate every 5th day
for i, (d, p) in enumerate(zip(future_dates[::5], future_predictions[::5])):
    axes[1].annotate(f'${p:.2f}', (d, p), xytext=(0,10), textcoords='offset points', ha='center')

plt.tight_layout()
plt.show()

# ---------- Print forecast ----------
forecast_df = pd.DataFrame({
    'Date': future_dates,
    'Predicted_Price': future_predictions,
    'Lower_CI': future_predictions - conf_int,
    'Upper_CI': future_predictions + conf_int
})

print("30-DAY SILVER PRICE FORECAST")
print("="*50)
print(forecast_df.head(10).to_string(index=False))
print(f"\nAverage predicted price: ${future_predictions.mean():.2f}")
print(f"Price range: ${future_predictions.min():.2f} - ${future_predictions.max():.2f}")

"""## **Step 13: Model Validation and Diagnostics**"""

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import jarque_bera, shapiro, probplot, norm
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.stats.diagnostic import acorr_ljungbox

# Use actual residuals
residuals = y_test - models['XGBoost'].predict(X_test)

fig, axes = plt.subplots(2, 2, figsize=(20, 12))

# 1. Residuals over time
axes[0,0].plot(dates_test, residuals, alpha=0.7, linewidth=1)
axes[0,0].axhline(0, color='red', linestyle='--', linewidth=2)
axes[0,0].set_title('XGBoost Model Residuals Over Time', fontsize=14, fontweight='bold')
axes[0,0].set_xlabel('Date')
axes[0,0].set_ylabel('Residuals ($)')
axes[0,0].grid(alpha=0.3)

# 2. Residuals distribution
axes[0,1].hist(residuals, bins=50, alpha=0.7, color='silver', edgecolor='black', density=True)
mu, sigma = norm.fit(residuals)
x = np.linspace(residuals.min(), residuals.max(), 100)
axes[0,1].plot(x, norm.pdf(x, mu, sigma), 'r-', lw=2, label=f'Normal fit (Î¼={mu:.2f}, Ïƒ={sigma:.2f})')
axes[0,1].set_title('Residuals Distribution', fontsize=14, fontweight='bold')
axes[0,1].set_xlabel('Residuals ($)')
axes[0,1].set_ylabel('Density')
axes[0,1].legend()

# 3. Q-Q plot
probplot(residuals, dist="norm", plot=axes[1,0])
axes[1,0].set_title('Q-Q Plot of Residuals', fontsize=14, fontweight='bold')
axes[1,0].grid(alpha=0.3)

# 4. Residuals autocorrelation
plot_acf(residuals, ax=axes[1,1], lags=30)
axes[1,1].set_title('Residuals Autocorrelation Function', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# ---------- Statistical tests ----------
print("MODEL DIAGNOSTICS")
print("="*50)

# Jarque-Bera
jb_stat, jb_pvalue = jarque_bera(residuals)
print(f"Jarque-Bera Test for Normality:\n  Statistic: {jb_stat:.4f}, p-value: {jb_pvalue:.4f}")
print(f"  {'Residuals are normally distributed' if jb_pvalue > 0.05 else 'Residuals are not normally distributed'}")

# Ljung-Box
lb_test = acorr_ljungbox(residuals, lags=10, return_df=True)
print(f"\nLjung-Box Test (first 10 lags):")
print(f"  p-values: {lb_test['lb_pvalue'].values}")
print(f"  {'No significant autocorrelation' if all(lb_test['lb_pvalue'] > 0.05) else 'Significant autocorrelation detected'}")

# Basic stats
print(f"\nResidual Statistics:\n  Mean: {residuals.mean():.4f}\n  Std: {residuals.std():.4f}\n  Min: {residuals.min():.4f}\n  Max: {residuals.max():.4f}")

"""## **Step 14: Final Summary and Key Insights**"""

# Ensure Year column exists
df_clean['Year'] = df_clean['Date'].dt.year

# Calculate 30-day rolling volatility
df_clean['Volatility_30'] = df_clean['Price'].rolling(30, min_periods=1).std()

print("ðŸ” SILVER PRICE TIME SERIES ANALYSIS - FINAL SUMMARY")
print("="*80)
print()

# Key Statistics Summary
print("ðŸ“Š KEY STATISTICS:")
print("-" * 40)
print(f"Dataset Period: {df_clean['Date'].min().strftime('%Y-%m-%d')} to {df_clean['Date'].max().strftime('%Y-%m-%d')}")
print(f"Total Trading Days: {len(df_clean):,}")
print(f"Price Range: ${df_clean['Price'].min():.2f} - ${df_clean['Price'].max():.2f}")
print(f"Average Price: ${df_clean['Price'].mean():.2f}")
print(f"Price Volatility (Std): ${df_clean['Price'].std():.2f}")
print(f"Total Return: {((df_clean['Price'].iloc[-1] / df_clean['Price'].iloc[0]) - 1) * 100:.1f}%")
print()

# Trend Analysis
print("ðŸ“ˆ TREND ANALYSIS:")
print("-" * 40)
yearly_growth = df_clean.groupby('Year')['Price'].mean().pct_change() * 100
print(f"Strongest Growth Year: {yearly_growth.idxmax()} ({yearly_growth.max():.1f}%)")
print(f"Weakest Growth Year: {yearly_growth.idxmin()} ({yearly_growth.min():.1f}%)")
print(f"Recent 5-Year Average Growth: {yearly_growth.tail(5).mean():.1f}% per year")
print()

# Model Performance Summary
print("ðŸ¤– MODEL PERFORMANCE:")
print("-" * 40)
# Use corrected keys from results dictionary
best_model_name = max(results.keys(), key=lambda x: results[x]['Test_R2'])
print(f"Best Model: {best_model_name}")
print(f"Best Model RÂ²: {results[best_model_name]['Test_R2']:.4f}")
print(f"Best Model RMSE: ${results[best_model_name]['Test_RMSE']:.2f}")
print()

# Feature Importance Summary
print("ðŸ”‘ KEY PREDICTIVE FEATURES:")
print("-" * 40)
top_features = xgb_importance.head(5)['feature'].tolist()
for i, feature in enumerate(top_features, 1):
    print(f"{i}. {feature}")
print()

# Forecast Summary
print("ðŸ”® 30-DAY FORECAST:")
print("-" * 40)
print(f"Predicted Price Range: ${future_predictions.min():.2f} - ${future_predictions.max():.2f}")
print(f"Average Predicted Price: ${future_predictions.mean():.2f}")
print(f"Current Price: ${df_clean['Price'].iloc[-1]:.2f}")
print(f"Expected Change: {((future_predictions.mean() / df_clean['Price'].iloc[-1]) - 1) * 100:.1f}%")
print()

# Market Insights
print("ðŸ’¡ KEY MARKET INSIGHTS:")
print("-" * 40)
print("âœ… Silver prices show strong upward trend over the analysis period")
print("âœ… Recent years (2024-2026) exhibit exponential growth patterns")
print("âœ… Seasonal effects exist but are relatively weak")
print("âœ… Recent price history is the strongest predictor of future prices")
print("âœ… Market volatility has increased significantly in recent periods")
print("âš ï¸  High volatility suggests increased market uncertainty")
print()

# Risk Assessment
print("âš ï¸  RISK ASSESSMENT:")
print("-" * 40)
high_vol_periods = df_clean[df_clean['Volatility_30'] > df_clean['Volatility_30'].quantile(0.9)]
print(f"High Volatility Periods (top 10%): {len(high_vol_periods)} days")
print(f"Maximum Single-Day Volatility: ${df_clean['Volatility_30'].max():.2f}")
print(f"Current Volatility Level: ${df_clean['Volatility_30'].iloc[-1]:.2f}")
print()

# Recommendations
print("ðŸŽ¯ RECOMMENDATIONS:")
print("-" * 40)
print("â€¢ XGBoost model recommended for short-term price forecasting")
print("â€¢ Monitor technical indicators (RSI, MACD) for trading signals")
print("â€¢ Consider volatility clustering in risk management")
print("â€¢ Regular model retraining recommended due to market evolution")
print("â€¢ Use confidence intervals for uncertainty quantification")

"""----"""